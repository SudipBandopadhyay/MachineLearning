{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bab5f7",
   "metadata": {},
   "source": [
    "## **Machine Learning Classification Challenge on Rain Data Set**\n",
    "Welcome to the machine learning challnege, in this scenario, you will be exploring classification on rain dataset\n",
    "\n",
    "**Note** -\n",
    "- Write the solution in this notebook by adding cells and follow the given instructions below. (Add additional notebook for coding if necessary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6fc1a",
   "metadata": {},
   "source": [
    "## Provided Dataset and Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef5176",
   "metadata": {},
   "source": [
    "\n",
    " - The Training dataset provided here is Rain_Prediction_train.csv has 20000 records with a target attribute as **RainTomorrow** containing values **yes** and **no** respectively. Use this dataset to train your model.\n",
    "\n",
    "\n",
    "- The testing set  Rain_Prediction_test.csv has 7908 records. \n",
    "\n",
    "\n",
    "- Here train the models by using four different algorithms namely - Logisitic Regression , Support Vector Machine , Decision Tree Classifier and Neural Networks (Multi-Layer Perceptron Classification) using scikit-learn package and save the models in the variables **search_log_reg** , **search_SVM** , **search_Decision** and **search_MLP** respectively.\n",
    "\n",
    "\n",
    "- The final score evaluation is based on Accuracy Score, Macro - F1 Score and Area Under the ROC Curve Score (ROC - AUC Score) on the prediction dataset.\n",
    "\n",
    "\n",
    "- **Hint** - Here try tuning the parameters of the models for better performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb9249",
   "metadata": {},
   "source": [
    "### Submission Details\n",
    "\n",
    "- Write the code to build the model and predicts the `RainTomorrow` (0 or 1) for the records in Rain_Prediction_test.csv file.\n",
    "\n",
    "\n",
    "\n",
    "- Once the predictions are ready for each models kindly save the predictions in the files named **Logistic_Predictions.csv**, **SVM_Predictions.csv**, **Decision_Predictions.csv** and **Neural_Predictions.csv**, which contains exactly 7908 rows. Each row contains the value of `RainTomorrow` (0 or 1) that corresponds to the record observations provided in the Rain_Prediction_test.csv file. \n",
    "\n",
    "\n",
    "\n",
    "- The submission prediction files should be of the below format.\n",
    "\n",
    "\n",
    "<img src=\"csv_format.PNG\">\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "- Here, run the first three cells to import the necessary packages ,setup the environment and to download the datasets, and final two cells to save the models as pickle files in the directory and to evaluate the results. (Here before evaluating the results kindly save the prediction csv files in the directory).\n",
    "\n",
    "\n",
    "\n",
    "- Here , this is the categorical codes for the target variable `RainTomorrow` -\n",
    "\n",
    " \n",
    "\n",
    "- no - 0,\n",
    "\n",
    "- yes - 1\n",
    "\n",
    "\n",
    "\n",
    "- Additional packages can be installed in the notebook using the command: \n",
    "\n",
    "\t\t !pip3 install --user package_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504d756",
   "metadata": {},
   "source": [
    "## Run the below cell to import the necessary packages required for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1150c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "#The below code will install imblearn package\n",
    "#!pip3 install imblearn\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9df2e0",
   "metadata": {},
   "source": [
    "## Run the below two cells to prepare the environmennt and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff354c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-09-18 14:10:56--  http://hrcdn.net/s3_pub/istreet-assets/hLgv0gBxC8WLHVPSteTk2A/Rain_Prediction_train.csv\n",
      "Resolving hrcdn.net (hrcdn.net)... 2600:140f:5::6011:c273, 2600:140f:5::6011:c252, 23.196.14.59, ...\n",
      "Connecting to hrcdn.net (hrcdn.net)|2600:140f:5::6011:c273|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://hrcdn.net/s3_pub/istreet-assets/hLgv0gBxC8WLHVPSteTk2A/Rain_Prediction_train.csv [following]\n",
      "--2021-09-18 14:10:56--  https://hrcdn.net/s3_pub/istreet-assets/hLgv0gBxC8WLHVPSteTk2A/Rain_Prediction_train.csv\n",
      "Connecting to hrcdn.net (hrcdn.net)|2600:140f:5::6011:c273|:443... connected.\n",
      "ERROR: cannot verify hrcdn.net's certificate, issued by 'CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "To connect to hrcdn.net insecurely, use `--no-check-certificate'.\n",
      "--2021-09-18 14:10:56--  http://hrcdn.net/s3_pub/istreet-assets/aDTstXYSDKlEHvzEFfQRRw/Rain_Prediction_test.csv\n",
      "Resolving hrcdn.net (hrcdn.net)... 2600:140f:5::6011:c273, 2600:140f:5::6011:c252, 23.196.14.59, ...\n",
      "Connecting to hrcdn.net (hrcdn.net)|2600:140f:5::6011:c273|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://hrcdn.net/s3_pub/istreet-assets/aDTstXYSDKlEHvzEFfQRRw/Rain_Prediction_test.csv [following]\n",
      "--2021-09-18 14:10:56--  https://hrcdn.net/s3_pub/istreet-assets/aDTstXYSDKlEHvzEFfQRRw/Rain_Prediction_test.csv\n",
      "Connecting to hrcdn.net (hrcdn.net)|2600:140f:5::6011:c273|:443... connected.\n",
      "ERROR: cannot verify hrcdn.net's certificate, issued by 'CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US':\n",
      "  Unable to locally verify the issuer's authority.\n",
      "To connect to hrcdn.net insecurely, use `--no-check-certificate'.\n"
     ]
    }
   ],
   "source": [
    "!wget hrcdn.net/s3_pub/istreet-assets/hLgv0gBxC8WLHVPSteTk2A/Rain_Prediction_train.csv\n",
    "\n",
    "!wget hrcdn.net/s3_pub/istreet-assets/aDTstXYSDKlEHvzEFfQRRw/Rain_Prediction_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d492b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\sudip\\sudip_ml\\datasciencee2\\sprint3_assessment\\classification-1.0-py2.py3-none-any.whl\n",
      "Classification is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Classification-1.0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cad785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here and add more cells for coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addcee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Rain_Prediction_train.csv')\n",
    "df_test = pd.read_csv('Rain_Prediction_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0c8723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/02/09</td>\n",
       "      <td>Williamtown</td>\n",
       "      <td>18.8</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>SSW</td>\n",
       "      <td>50.0</td>\n",
       "      <td>S</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.6</td>\n",
       "      <td>24.1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22/08/11</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>16.3</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>E</td>\n",
       "      <td>50.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1017.8</td>\n",
       "      <td>1013.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>31.2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21/08/13</td>\n",
       "      <td>Moree</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>SSW</td>\n",
       "      <td>39.0</td>\n",
       "      <td>SSW</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>1016.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>15.3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29/10/10</td>\n",
       "      <td>Mildura</td>\n",
       "      <td>14.7</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>W</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1010.2</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>30.8</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08/06/14</td>\n",
       "      <td>Cairns</td>\n",
       "      <td>18.4</td>\n",
       "      <td>20.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SSW</td>\n",
       "      <td>35.0</td>\n",
       "      <td>S</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>1014.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>20.1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>14/08/14</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>6.4</td>\n",
       "      <td>19.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10.3</td>\n",
       "      <td>SSE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>1032.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>25/08/10</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>W</td>\n",
       "      <td>35.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1013.8</td>\n",
       "      <td>1010.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.4</td>\n",
       "      <td>18.3</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>29/10/11</td>\n",
       "      <td>Mildura</td>\n",
       "      <td>14.5</td>\n",
       "      <td>23.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>9.6</td>\n",
       "      <td>WSW</td>\n",
       "      <td>43.0</td>\n",
       "      <td>SSW</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1014.7</td>\n",
       "      <td>1014.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>22.8</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>25/01/15</td>\n",
       "      <td>Watsonia</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>W</td>\n",
       "      <td>50.0</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>1008.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>23/05/11</td>\n",
       "      <td>Nuriootpa</td>\n",
       "      <td>10.1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>WSW</td>\n",
       "      <td>56.0</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.4</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Location  MinTemp  MaxTemp  Rainfall  Evaporation  \\\n",
       "0      09/02/09  Williamtown     18.8     26.3       0.0         10.0   \n",
       "1      22/08/11       Darwin     16.3     31.8       0.0          8.6   \n",
       "2      21/08/13        Moree     -1.1     15.9       0.0          5.0   \n",
       "3      29/10/10      Mildura     14.7     32.8       0.0          7.2   \n",
       "4      08/06/14       Cairns     18.4     20.3      15.8          3.8   \n",
       "...         ...          ...      ...      ...       ...          ...   \n",
       "19995  14/08/14       Sydney      6.4     19.3       0.2          2.8   \n",
       "19996  25/08/10     Brisbane     15.0     18.6       0.2          5.6   \n",
       "19997  29/10/11      Mildura     14.5     23.9       3.6          7.8   \n",
       "19998  25/01/15     Watsonia     14.0     23.7       0.0          8.2   \n",
       "19999  23/05/11    Nuriootpa     10.1     12.4      22.0          2.0   \n",
       "\n",
       "       Sunshine WindGustDir  WindGustSpeed WindDir9am  ... WindSpeed3pm  \\\n",
       "0           9.7         SSW           50.0          S  ...         37.0   \n",
       "1          11.3           E           50.0         SE  ...         28.0   \n",
       "2          10.9         SSW           39.0        SSW  ...         20.0   \n",
       "3           7.3           W           61.0        NNE  ...         26.0   \n",
       "4           0.0         SSW           35.0          S  ...         11.0   \n",
       "...         ...         ...            ...        ...  ...          ...   \n",
       "19995      10.3         SSE           30.0          W  ...         22.0   \n",
       "19996       0.0           W           35.0          W  ...          9.0   \n",
       "19997       9.6         WSW           43.0        SSW  ...         22.0   \n",
       "19998      11.0           W           50.0        WSW  ...         26.0   \n",
       "19999       3.9         WSW           56.0        WSW  ...         33.0   \n",
       "\n",
       "       Humidity9am  Humidity3pm  Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  \\\n",
       "0             84.0         72.0       1010.8       1010.1       6.0       3.0   \n",
       "1             30.0         11.0       1017.8       1013.4       3.0       2.0   \n",
       "2             53.0         23.0       1020.3       1016.4       1.0       1.0   \n",
       "3             34.0         16.0       1010.2       1006.0       6.0       7.0   \n",
       "4             90.0         94.0       1016.8       1014.5       8.0       8.0   \n",
       "...            ...          ...          ...          ...       ...       ...   \n",
       "19995         64.0         49.0       1035.0       1032.5       1.0       2.0   \n",
       "19996         61.0         53.0       1013.8       1010.4       8.0       8.0   \n",
       "19997         77.0         31.0       1014.7       1014.8       7.0       4.0   \n",
       "19998         62.0         38.0       1007.1       1008.5       7.0       5.0   \n",
       "19999         94.0         64.0       1010.0       1010.6       7.0       6.0   \n",
       "\n",
       "       Temp9am  Temp3pm  RainTomorrow  \n",
       "0         22.6     24.1            No  \n",
       "1         23.2     31.2            No  \n",
       "2          7.9     15.3            No  \n",
       "3         23.2     30.8           Yes  \n",
       "4         19.5     20.1           Yes  \n",
       "...        ...      ...           ...  \n",
       "19995      9.5     18.1            No  \n",
       "19996     15.4     18.3            No  \n",
       "19997     15.3     22.8            No  \n",
       "19998     16.1     20.0            No  \n",
       "19999     10.6     10.4           Yes  \n",
       "\n",
       "[20000 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea48a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['RainTomorrow'] == 'No', 'RainTomorrow'] = 0\n",
    "df.loc[df['RainTomorrow'] == 'Yes', 'RainTomorrow'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26a5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RainTomorrow      0     1    All\n",
      "Date                            \n",
      "01/01/09          8     1      9\n",
      "01/01/10          5     2      7\n",
      "01/01/11          6     2      8\n",
      "01/01/12          3     1      4\n",
      "01/01/13          6     1      7\n",
      "...             ...   ...    ...\n",
      "31/12/13          3     0      3\n",
      "31/12/14          5     0      5\n",
      "31/12/15          2     0      2\n",
      "31/12/16          2     0      2\n",
      "All           15608  4392  20000\n",
      "\n",
      "[3238 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "contingency_table = pd.crosstab(df['Date'], df['RainTomorrow'],margins=True)\n",
    "print(contingency_table)\n",
    "#contingency_table.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cccdb683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              object\n",
       "Location          object\n",
       "MinTemp          float64\n",
       "MaxTemp          float64\n",
       "Rainfall         float64\n",
       "Evaporation      float64\n",
       "Sunshine         float64\n",
       "WindGustDir       object\n",
       "WindGustSpeed    float64\n",
       "WindDir9am        object\n",
       "WindDir3pm        object\n",
       "WindSpeed9am     float64\n",
       "WindSpeed3pm     float64\n",
       "Humidity9am      float64\n",
       "Humidity3pm      float64\n",
       "Pressure9am      float64\n",
       "Pressure3pm      float64\n",
       "Cloud9am         float64\n",
       "Cloud3pm         float64\n",
       "Temp9am          float64\n",
       "Temp3pm          float64\n",
       "RainTomorrow      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.RainTomorrow.value_counts()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1fb6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df1=df\n",
    "    df1= pd.get_dummies (df1, columns =['Location'], drop_first=True)\n",
    "    df1 = pd.get_dummies (df1, columns =['WindGustDir'], drop_first=True)\n",
    "    df1 = pd.get_dummies(df1, columns =['WindDir9am'], drop_first=True)\n",
    "    df1 = pd.get_dummies(df1, columns =['WindDir3pm'], drop_first=True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a6b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.drop('Date',axis=1)\n",
    "df2 = preprocessing(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1424aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 86)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "X= df2.iloc[:,:-1]\n",
    "y=df2.RainTomorrow\n",
    "y=y.astype('int')\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df1e896",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a541d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(sampling_strategy='minority')\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1152b97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = LogisticRegression()\\nsolvers = [\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\']\\npenalty = [\\'l2\\']\\nc_values = [100, 10, 1.0, 0.1, 0.01]\\n# define grid search\\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=\\'f1_macro\\',error_score=0)\\ngrid_result = grid_search.fit(X_train, y_train)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\n#Best: 1.000000 using {\\'C\\': 100, \\'penalty\\': \\'l2\\', \\'solver\\': \\'newton-cg\\'}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1_macro',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#Best: 1.000000 using {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bea1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sudip\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "search_log_reg = LogisticRegression(C=.1,penalty= 'l2', solver= 'lbfgs',random_state=42)\n",
    "search_log_reg.fit(X_train, y_train)\n",
    "predict_log_reg= search_log_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33efa52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8846969696969696\n",
      "F1 Macro: 0.8426754088846578\n",
      "Confusion Matrix: [[4625  544]\n",
      " [ 217 1214]]\n",
      "Sensitivity: 0.8947572064229058\n",
      "Specificity: 0.8483577917540182\n"
     ]
    }
   ],
   "source": [
    "income_acc_lr = accuracy_score(y_test,predict_log_reg)\n",
    "income_f1_macro_lr = f1_score(y_test,predict_log_reg,average='macro')\n",
    "income_confusion_lr=confusion_matrix(y_test,predict_log_reg)\n",
    "total1=sum(sum(income_confusion_lr))\n",
    "income_sensitivity_lr=income_confusion_lr[0,0]/(income_confusion_lr[0,0]+income_confusion_lr[0,1])\n",
    "income_specificity_lr=income_confusion_lr[1,1]/(income_confusion_lr[1,0]+income_confusion_lr[1,1])\n",
    "\n",
    "print('Accuracy:',income_acc_lr)\n",
    "print('F1 Macro:',income_f1_macro_lr)\n",
    "print('Confusion Matrix:',income_confusion_lr)\n",
    "print('Sensitivity:',income_sensitivity_lr)\n",
    "print('Specificity:',income_specificity_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5c25dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = SVC()\\nkernel = [\\'poly\\', \\'rbf\\', \\'sigmoid\\']\\nC = [50, 10, 1.0, 0.1, 0.01]\\ngamma = [\\'scale\\']\\n# define grid search\\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=\\'f1_macro\\',error_score=0)\\ngrid_result = grid_search.fit(X_train, y_train)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\n#Best: 0.851181 using {\\'C\\': 50, \\'gamma\\': \\'scale\\', \\'kernel\\': \\'poly\\'}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = SVC()\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1_macro',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#Best: 0.851181 using {'C': 50, 'gamma': 'scale', 'kernel': 'poly'}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f98786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_SVM=SVC(C= 10, gamma='scale', kernel= 'poly',random_state=42)\n",
    "search_SVM.fit(X_train, y_train)\n",
    "predict_SVM= search_SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b0e9ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8133333333333334\n",
      "F1 Macro: 0.7595252682420632\n",
      "Confusion Matrix: [[4245  924]\n",
      " [ 308 1123]]\n",
      "Sensitivity: 0.8212420197330238\n",
      "Specificity: 0.7847658979734451\n"
     ]
    }
   ],
   "source": [
    "income_acc_SVM = accuracy_score(y_test,predict_SVM)\n",
    "income_f1_macro_SVM = f1_score(y_test,predict_SVM,average='macro')\n",
    "income_confusion_SVM=confusion_matrix(y_test,predict_SVM)\n",
    "total1=sum(sum(income_confusion_SVM))\n",
    "income_sensitivity_SVM=income_confusion_SVM[0,0]/(income_confusion_SVM[0,0]+income_confusion_SVM[0,1])\n",
    "income_specificity_SVM=income_confusion_SVM[1,1]/(income_confusion_SVM[1,0]+income_confusion_SVM[1,1])\n",
    "\n",
    "print('Accuracy:',income_acc_SVM)\n",
    "print('F1 Macro:',income_f1_macro_SVM)\n",
    "print('Confusion Matrix:',income_confusion_SVM)\n",
    "print('Sensitivity:',income_sensitivity_SVM)\n",
    "print('Specificity:',income_specificity_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bb9d6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = DecisionTreeClassifier()\\nparams = {\\n    \\'max_depth\\': [2, 3, 5, 10, 20],\\n    \\'min_samples_leaf\\': [5, 10, 20, 50, 100],\\n    \\'criterion\\': [\"gini\", \"entropy\"]\\n}\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\\ngrid_search = GridSearchCV(estimator=model, param_grid=params, cv=cv, n_jobs=-1, verbose=1, scoring = \"f1_macro\")\\ngrid_result=clf.fit(X_train, y_train)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = DecisionTreeClassifier()\n",
    "params = {\n",
    "    'max_depth': [2, 3, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=params, cv=cv, n_jobs=-1, verbose=1, scoring = \"f1_macro\")\n",
    "grid_result=clf.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "942aaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_Decision = DecisionTreeClassifier(criterion= 'gini', max_depth= 2, min_samples_leaf= 5,random_state=42)\n",
    "search_Decision.fit(X_train, y_train)\n",
    "predict_Decision= search_Decision.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ad148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eccf235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F1 Macro: 1.0\n",
      "Confusion Matrix: [[5169    0]\n",
      " [   0 1431]]\n",
      "Sensitivity: 1.0\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "income_acc_Decision = accuracy_score(y_test,predict_Decision)\n",
    "income_f1_macro_Decision = f1_score(y_test,predict_Decision,average='macro')\n",
    "income_confusion_Decision=confusion_matrix(y_test,predict_Decision)\n",
    "total1=sum(sum(income_confusion_Decision))\n",
    "income_sensitivity_Decision=income_confusion_Decision[0,0]/(income_confusion_Decision[0,0]+income_confusion_Decision[0,1])\n",
    "income_specificity_Decision=income_confusion_Decision[1,1]/(income_confusion_Decision[1,0]+income_confusion_Decision[1,1])\n",
    "\n",
    "print('Accuracy:',income_acc_Decision)\n",
    "print('F1 Macro:',income_f1_macro_Decision)\n",
    "print('Confusion Matrix:',income_confusion_Decision)\n",
    "print('Sensitivity:',income_sensitivity_Decision)\n",
    "print('Specificity:',income_specificity_Decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45152bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlp = MLPClassifier(max_iter=100)\\nparameter_space = {\\n    \\'hidden_layer_sizes\\': [(50,50,50), (50,100,50), (100,)],\\n    \\'activation\\': [\\'tanh\\', \\'relu\\'],\\n    \\'solver\\': [\\'sgd\\', \\'adam\\'],\\n    \\'alpha\\': [0.0001, 0.05],\\n    \\'learning_rate\\': [\\'constant\\',\\'adaptive\\'],\\n}\\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\\ngrid_search = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring=\\'f1_macro\\',error_score=0)\\ngrid_result=grid_search.fit(X_train, y_train)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\\n#Best: 1.000000 using {\\'activation\\': \\'tanh\\', \\'alpha\\': 0.0001, \\'hidden_layer_sizes\\': (100,), \\'learning_rate\\': \\'constant\\', \\'solver\\': \\'adam\\'}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''mlp = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5, scoring='f1_macro',error_score=0)\n",
    "grid_result=grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#Best: 1.000000 using {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f739610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''search_MLP = MLPClassifier(activation= 'logistic', alpha=1, \n",
    "                           hidden_layer_sizes= (100,200), learning_rate='adaptive', solver= 'adam',random_state=42)'''\n",
    "search_MLP = MLPClassifier(activation= 'logistic', alpha=5 ,\n",
    "                           hidden_layer_sizes= (100,200), learning_rate='adaptive', solver= 'adam',random_state=42)\n",
    "search_MLP.fit(X_train, y_train)\n",
    "predict_MLP= search_MLP.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2832255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7790909090909091\n",
      "F1 Macro: 0.7344523489492837\n",
      "Confusion Matrix: [[3924 1245]\n",
      " [ 213 1218]]\n",
      "Sensitivity: 0.759141033081834\n",
      "Specificity: 0.8511530398322851\n"
     ]
    }
   ],
   "source": [
    "income_acc_MLP = accuracy_score(y_test,predict_MLP)\n",
    "income_f1_macro_MLP = f1_score(y_test,predict_MLP,average='macro')\n",
    "income_confusion_MLP=confusion_matrix(y_test,predict_MLP)\n",
    "total1=sum(sum(income_confusion_MLP))\n",
    "income_sensitivity_MLP=income_confusion_MLP[0,0]/(income_confusion_MLP[0,0]+income_confusion_MLP[0,1])\n",
    "income_specificity_MLP=income_confusion_MLP[1,1]/(income_confusion_MLP[1,0]+income_confusion_MLP[1,1])\n",
    "\n",
    "print('Accuracy:',income_acc_MLP)\n",
    "print('F1 Macro:',income_f1_macro_MLP)\n",
    "print('Confusion Matrix:',income_confusion_MLP)\n",
    "print('Sensitivity:',income_sensitivity_MLP)\n",
    "print('Specificity:',income_specificity_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8700cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df_test.drop('Date',axis=1)\n",
    "df3 = preprocessing(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9a6aff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log_reg_test=search_log_reg.predict(df3)\n",
    "predict_Decision_test= search_Decision.predict(df3)\n",
    "predict_SVM_test=search_SVM.predict(df3)\n",
    "predict_MLP_test=search_MLP.predict(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "622f310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm 0    7502\n",
      "1     406\n",
      "Name: RainTomorrow, dtype: int64\n",
      "mlp 0    4897\n",
      "1    3011\n",
      "Name: RainTomorrow, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "output_lr = pd.DataFrame(predict_log_reg_test)\n",
    "#output_lr.index += 1 \n",
    "output_SVM = pd.DataFrame(predict_Decision_test)\n",
    "#output_SVM.index += 1 \n",
    "output_dt = pd.DataFrame(predict_SVM_test)\n",
    "#output_dt.index += 1 \n",
    "output_mlp = pd.DataFrame(predict_MLP_test)\n",
    "#output_mlp.index += 1 \n",
    "output_lr.columns = ['RainTomorrow']\n",
    "output_SVM.columns = ['RainTomorrow']\n",
    "output_dt.columns = ['RainTomorrow']\n",
    "output_mlp.columns = ['RainTomorrow']\n",
    "\n",
    "output_lr.to_csv('Logistic_Predictions.csv',header=True,index=False)\n",
    "output_SVM.to_csv('SVM_Predictions.csv',header=True,index=False)\n",
    "output_dt.to_csv('Decision_Predictions.csv',header=True,index=False)\n",
    "output_mlp.to_csv('Neural_Predictions.csv',header=True,index=False)\n",
    "#print('dt',output_dt.RainTomorrow.value_counts())\n",
    "#print('lr',output_lr.RainTomorrow.value_counts())\n",
    "print('svm',output_SVM.RainTomorrow.value_counts())\n",
    "print('mlp',output_mlp.RainTomorrow.value_counts())\n",
    "#Logistic_Predictions.csv, SVM_Predictions.csv, Decision_Predictions.csv and Neural_Predictions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5301c",
   "metadata": {},
   "source": [
    "## Run the below cell to save the models for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3cc1886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#save the Logistic Regression model \n",
    "filename = 'Log_Reg.pkl'\n",
    "pickle.dump(search_log_reg, open(filename, 'wb'))\n",
    "\n",
    "#save the Support Vector Machine model \n",
    "filename = 'SVM.pkl'\n",
    "pickle.dump(search_SVM, open(filename, 'wb'))\n",
    "\n",
    "#save the Decision Tree Classsifier model \n",
    "filename = 'Decision.pkl'\n",
    "pickle.dump(search_Decision, open(filename, 'wb'))\n",
    "\n",
    "#save the Neural Network model \n",
    "filename = 'Neural.pkl'\n",
    "pickle.dump(search_MLP, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033ab64",
   "metadata": {},
   "source": [
    "## Run the below cell to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d393b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kindly check your Support Vector Machine Model\n",
      "FS_SCORE:65%\n"
     ]
    }
   ],
   "source": [
    "from Classification import evaluate\n",
    "evaluate.evaluation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c6d2e",
   "metadata": {},
   "source": [
    "# All the best. May your models predict as accurately as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cfefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d32e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
